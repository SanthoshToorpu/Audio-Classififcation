{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1jKku2atmyRE_Z79bZCSLKR1HmLhNtAlo",
      "authorship_tag": "ABX9TyMCi/UEfh+P9GcMgsUFEQp0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanthoshToorpu/Audio-Classififcation/blob/main/Bigram_unoptimized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('/content/drive/MyDrive/pride_and_prejudice.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "792fbf0d-0d72-41dc-83df-a36ffabc1333"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  726907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "29bf7938-968c-4800-f747-550a6d17e8c1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREFACE.\n",
            "\n",
            "\n",
            "_Walt Whitman has somewhere a fine and just distinction between “loving\n",
            "by allowance” and “loving with personal love.” This distinction applies\n",
            "to books as well as to men and women; and in the case of the not very\n",
            "numerous authors who are the objects of the personal affection, it\n",
            "brings a curious consequence with it. There is much more difference as\n",
            "to their best work than in the case of those others who are loved “by\n",
            "allowance” by convention, and because it is felt to be the right and\n",
            "proper thing to love them. And in the sect--fairly large and yet\n",
            "unusually choice--of Austenians or Janites, there would probably be\n",
            "found partisans of the claim to primacy of almost every one of the\n",
            "novels. To some the delightful freshness and humour of_ Northanger\n",
            "Abbey, _its completeness, finish, and_ entrain, _obscure the undoubted\n",
            "critical facts that its scale is small, and its scheme, after all, that\n",
            "of burlesque or parody, a kind in which the first rank is reached with\n",
            "difficulty._ Pers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "790850c8-c0b9-44c5-cf93-7d3a6c98acf1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !&()*,-./0123456789:;?ABCDEFGHIJKLMNOPRSTUVWXYZ[]^_abcdefghijklmnopqrstuvwxyz{}·àâéêœ‘’“”\n",
            "91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "50127be9-1c4f-4f94-8d33-a8c7579c41c2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[60, 61, 61, 1, 72, 60, 57, 70, 57]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "a90f427f-63e4-4bd0-e560-a7402d8cc4d0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([726907]) torch.int64\n",
            "tensor([39, 40, 28, 29, 24, 26, 28,  9,  0,  0,  0, 52, 45, 53, 64, 72,  1, 45,\n",
            "        60, 61, 72, 65, 53, 66,  1, 60, 53, 71,  1, 71, 67, 65, 57, 75, 60, 57,\n",
            "        70, 57,  1, 53,  1, 58, 61, 66, 57,  1, 53, 66, 56,  1, 62, 73, 71, 72,\n",
            "         1, 56, 61, 71, 72, 61, 66, 55, 72, 61, 67, 66,  1, 54, 57, 72, 75, 57,\n",
            "        57, 66,  1, 89, 64, 67, 74, 61, 66, 59,  0, 54, 77,  1, 53, 64, 64, 67,\n",
            "        75, 53, 66, 55, 57, 90,  1, 53, 66, 56,  1, 89, 64, 67, 74, 61, 66, 59,\n",
            "         1, 75, 61, 72, 60,  1, 68, 57, 70, 71, 67, 66, 53, 64,  1, 64, 67, 74,\n",
            "        57,  9, 90,  1, 42, 60, 61, 71,  1, 56, 61, 71, 72, 61, 66, 55, 72, 61,\n",
            "        67, 66,  1, 53, 68, 68, 64, 61, 57, 71,  0, 72, 67,  1, 54, 67, 67, 63,\n",
            "        71,  1, 53, 71,  1, 75, 57, 64, 64,  1, 53, 71,  1, 72, 67,  1, 65, 57,\n",
            "        66,  1, 53, 66, 56,  1, 75, 67, 65, 57, 66, 22,  1, 53, 66, 56,  1, 61,\n",
            "        66,  1, 72, 60, 57,  1, 55, 53, 71, 57,  1, 67, 58,  1, 72, 60, 57,  1,\n",
            "        66, 67, 72,  1, 74, 57, 70, 77,  0, 66, 73, 65, 57, 70, 67, 73, 71,  1,\n",
            "        53, 73, 72, 60, 67, 70, 71,  1, 75, 60, 67,  1, 53, 70, 57,  1, 72, 60,\n",
            "        57,  1, 67, 54, 62, 57, 55, 72, 71,  1, 67, 58,  1, 72, 60, 57,  1, 68,\n",
            "        57, 70, 71, 67, 66, 53, 64,  1, 53, 58, 58, 57, 55, 72, 61, 67, 66,  7,\n",
            "         1, 61, 72,  0, 54, 70, 61, 66, 59, 71,  1, 53,  1, 55, 73, 70, 61, 67,\n",
            "        73, 71,  1, 55, 67, 66, 71, 57, 69, 73, 57, 66, 55, 57,  1, 75, 61, 72,\n",
            "        60,  1, 61, 72,  9,  1, 42, 60, 57, 70, 57,  1, 61, 71,  1, 65, 73, 55,\n",
            "        60,  1, 65, 67, 70, 57,  1, 56, 61, 58, 58, 57, 70, 57, 66, 55, 57,  1,\n",
            "        53, 71,  0, 72, 67,  1, 72, 60, 57, 61, 70,  1, 54, 57, 71, 72,  1, 75,\n",
            "        67, 70, 63,  1, 72, 60, 53, 66,  1, 61, 66,  1, 72, 60, 57,  1, 55, 53,\n",
            "        71, 57,  1, 67, 58,  1, 72, 60, 67, 71, 57,  1, 67, 72, 60, 57, 70, 71,\n",
            "         1, 75, 60, 67,  1, 53, 70, 57,  1, 64, 67, 74, 57, 56,  1, 89, 54, 77,\n",
            "         0, 53, 64, 64, 67, 75, 53, 66, 55, 57, 90,  1, 54, 77,  1, 55, 67, 66,\n",
            "        74, 57, 66, 72, 61, 67, 66,  7,  1, 53, 66, 56,  1, 54, 57, 55, 53, 73,\n",
            "        71, 57,  1, 61, 72,  1, 61, 71,  1, 58, 57, 64, 72,  1, 72, 67,  1, 54,\n",
            "        57,  1, 72, 60, 57,  1, 70, 61, 59, 60, 72,  1, 53, 66, 56,  0, 68, 70,\n",
            "        67, 68, 57, 70,  1, 72, 60, 61, 66, 59,  1, 72, 67,  1, 64, 67, 74, 57,\n",
            "         1, 72, 60, 57, 65,  9,  1, 24, 66, 56,  1, 61, 66,  1, 72, 60, 57,  1,\n",
            "        71, 57, 55, 72,  8,  8, 58, 53, 61, 70, 64, 77,  1, 64, 53, 70, 59, 57,\n",
            "         1, 53, 66, 56,  1, 77, 57, 72,  0, 73, 66, 73, 71, 73, 53, 64, 64, 77,\n",
            "         1, 55, 60, 67, 61, 55, 57,  8,  8, 67, 58,  1, 24, 73, 71, 72, 57, 66,\n",
            "        61, 53, 66, 71,  1, 67, 70,  1, 33, 53, 66, 61, 72, 57, 71,  7,  1, 72,\n",
            "        60, 57, 70, 57,  1, 75, 67, 73, 64, 56,  1, 68, 70, 67, 54, 53, 54, 64,\n",
            "        77,  1, 54, 57,  0, 58, 67, 73, 66, 56,  1, 68, 53, 70, 72, 61, 71, 53,\n",
            "        66, 71,  1, 67, 58,  1, 72, 60, 57,  1, 55, 64, 53, 61, 65,  1, 72, 67,\n",
            "         1, 68, 70, 61, 65, 53, 55, 77,  1, 67, 58,  1, 53, 64, 65, 67, 71, 72,\n",
            "         1, 57, 74, 57, 70, 77,  1, 67, 66, 57,  1, 67, 58,  1, 72, 60, 57,  0,\n",
            "        66, 67, 74, 57, 64, 71,  9,  1, 42, 67,  1, 71, 67, 65, 57,  1, 72, 60,\n",
            "        57,  1, 56, 57, 64, 61, 59, 60, 72, 58, 73, 64,  1, 58, 70, 57, 71, 60,\n",
            "        66, 57, 71, 71,  1, 53, 66, 56,  1, 60, 73, 65, 67, 73, 70,  1, 67, 58,\n",
            "        52,  1, 37, 67, 70, 72, 60, 53, 66, 59, 57, 70,  0, 24, 54, 54, 57, 77,\n",
            "         7,  1, 52, 61, 72, 71,  1, 55, 67, 65, 68, 64, 57, 72, 57, 66, 57, 71,\n",
            "        71,  7,  1, 58, 61, 66, 61, 71, 60,  7,  1, 53, 66, 56, 52,  1, 57, 66,\n",
            "        72, 70, 53, 61, 66,  7,  1, 52, 67, 54, 71, 55, 73, 70, 57,  1, 72, 60,\n",
            "        57,  1, 73, 66, 56, 67, 73, 54, 72, 57, 56,  0, 55, 70, 61, 72, 61, 55,\n",
            "        53, 64,  1, 58, 53, 55, 72, 71,  1, 72, 60, 53, 72,  1, 61, 72, 71,  1,\n",
            "        71, 55, 53, 64, 57,  1, 61, 71,  1, 71, 65, 53, 64, 64,  7,  1, 53, 66,\n",
            "        56,  1, 61, 72, 71,  1, 71, 55, 60, 57, 65, 57,  7,  1, 53, 58, 72, 57,\n",
            "        70,  1, 53, 64, 64,  7,  1, 72, 60, 53, 72,  0, 67, 58,  1, 54, 73, 70,\n",
            "        64, 57, 71, 69, 73, 57,  1, 67, 70,  1, 68, 53, 70, 67, 56, 77,  7,  1,\n",
            "        53,  1, 63, 61, 66, 56,  1, 61, 66,  1, 75, 60, 61, 55, 60,  1, 72, 60,\n",
            "        57,  1, 58, 61, 70, 71, 72,  1, 70, 53, 66, 63,  1, 61, 71,  1, 70, 57,\n",
            "        53, 55, 60, 57, 56,  1, 75, 61, 72, 60,  0, 56, 61, 58, 58, 61, 55, 73,\n",
            "        64, 72, 77,  9, 52,  1, 39, 57, 70, 71])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "de79b558-63b3-4bcb-e214-f1618a8da97b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([39, 40, 28, 29, 24, 26, 28,  9,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **1. Slice the training data:**\n",
        "x = train_data[:block_size]  # Create a slice of `block_size` elements from the beginning of `train_data`\n",
        "y = train_data[1:block_size+1]  # Create a slice starting from the second element to the `block_size+1` element\n",
        "\n",
        "# **2. Iterate through the blocks:**\n",
        "for t in range(block_size):\n",
        "    # **3. Construct context and target:**\n",
        "    context = x[:t+1]  # Subset of `x` up to (and including) the current index `t`\n",
        "    target = y[t]  # Element at index `t` in `y`\n",
        "\n",
        "    # **4. Print context and target:**\n",
        "    print(f\"when input is {context} the target: {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "ad27401a-8147-4ae5-f6cb-8d9d61a7cc38"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([39]) the target: 40\n",
            "when input is tensor([39, 40]) the target: 28\n",
            "when input is tensor([39, 40, 28]) the target: 29\n",
            "when input is tensor([39, 40, 28, 29]) the target: 24\n",
            "when input is tensor([39, 40, 28, 29, 24]) the target: 26\n",
            "when input is tensor([39, 40, 28, 29, 24, 26]) the target: 28\n",
            "when input is tensor([39, 40, 28, 29, 24, 26, 28]) the target: 9\n",
            "when input is tensor([39, 40, 28, 29, 24, 26, 28,  9]) the target: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **1. Set seed for reproducibility:**\n",
        "torch.manual_seed(1337)  # Ensure consistent random number generation\n",
        "\n",
        "# **2. Define hyperparameters:**\n",
        "batch_size = 4  # Number of sequences processed in parallel\n",
        "block_size = 8  # Maximum context length for predictions\n",
        "\n",
        "# **3. Function to generate batches of data:**\n",
        "def get_batch(split):\n",
        "    # Select appropriate dataset based on split\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    # Randomly select batch_size starting indices for sequences\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    # Create input and target sequences using stacking\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "    # Return input and target batches\n",
        "    return x, y\n",
        "\n",
        "# **4. Generate a batch of training data:**\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "# **5. Print input and target shapes and values:**\n",
        "print('inputs:')\n",
        "print(xb.shape)  # Print dimensions of input batch\n",
        "print(xb)        # Print actual input batch values\n",
        "print('targets:')\n",
        "print(yb.shape)  # Print dimensions of target batch\n",
        "print(yb)        # Print actual target batch values\n",
        "\n",
        "# **6. Iterate through the batch and time dimensions:**\n",
        "print('----')\n",
        "for b in range(batch_size):  # Loop through each sequence in the batch\n",
        "    for t in range(block_size):  # Loop through each time step in the sequence\n",
        "        # Extract context and target for current position\n",
        "        context = xb[b, :t+1]  # Slice input up to current time step\n",
        "        target = yb[b, t]      # Corresponding target element\n",
        "\n",
        "        # Print context and target\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "45ee2298-26d8-473c-97ae-d4728cd312cd"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 8,  0,  0, 89, 32,  1, 60, 53],\n",
            "        [ 9,  1, 42, 60, 53, 72,  1, 75],\n",
            "        [70, 53, 54, 64, 57,  9, 90,  0],\n",
            "        [61, 72,  1, 61, 71,  1, 62, 73]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 0,  0, 89, 32,  1, 60, 53, 74],\n",
            "        [ 1, 42, 60, 53, 72,  1, 75, 61],\n",
            "        [53, 54, 64, 57,  9, 90,  0,  0],\n",
            "        [72,  1, 61, 71,  1, 62, 73, 71]])\n",
            "----\n",
            "when input is [8] the target: 0\n",
            "when input is [8, 0] the target: 0\n",
            "when input is [8, 0, 0] the target: 89\n",
            "when input is [8, 0, 0, 89] the target: 32\n",
            "when input is [8, 0, 0, 89, 32] the target: 1\n",
            "when input is [8, 0, 0, 89, 32, 1] the target: 60\n",
            "when input is [8, 0, 0, 89, 32, 1, 60] the target: 53\n",
            "when input is [8, 0, 0, 89, 32, 1, 60, 53] the target: 74\n",
            "when input is [9] the target: 1\n",
            "when input is [9, 1] the target: 42\n",
            "when input is [9, 1, 42] the target: 60\n",
            "when input is [9, 1, 42, 60] the target: 53\n",
            "when input is [9, 1, 42, 60, 53] the target: 72\n",
            "when input is [9, 1, 42, 60, 53, 72] the target: 1\n",
            "when input is [9, 1, 42, 60, 53, 72, 1] the target: 75\n",
            "when input is [9, 1, 42, 60, 53, 72, 1, 75] the target: 61\n",
            "when input is [70] the target: 53\n",
            "when input is [70, 53] the target: 54\n",
            "when input is [70, 53, 54] the target: 64\n",
            "when input is [70, 53, 54, 64] the target: 57\n",
            "when input is [70, 53, 54, 64, 57] the target: 9\n",
            "when input is [70, 53, 54, 64, 57, 9] the target: 90\n",
            "when input is [70, 53, 54, 64, 57, 9, 90] the target: 0\n",
            "when input is [70, 53, 54, 64, 57, 9, 90, 0] the target: 0\n",
            "when input is [61] the target: 72\n",
            "when input is [61, 72] the target: 1\n",
            "when input is [61, 72, 1] the target: 61\n",
            "when input is [61, 72, 1, 61] the target: 71\n",
            "when input is [61, 72, 1, 61, 71] the target: 1\n",
            "when input is [61, 72, 1, 61, 71, 1] the target: 62\n",
            "when input is [61, 72, 1, 61, 71, 1, 62] the target: 73\n",
            "when input is [61, 72, 1, 61, 71, 1, 62, 73] the target: 71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "061a0b1b-c1ee-4bb3-997e-a800d1c87781"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 8,  0,  0, 89, 32,  1, 60, 53],\n",
            "        [ 9,  1, 42, 60, 53, 72,  1, 75],\n",
            "        [70, 53, 54, 64, 57,  9, 90,  0],\n",
            "        [61, 72,  1, 61, 71,  1, 62, 73]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "8c385f28-b330-4cca-9693-b310ad88bdc8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 91])\n",
            "tensor(4.5440, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "RàYb?LtyjP3Ae3JVlElp0UYy”6hKD}:hs7’iMêRkwkdbli47-”:NiyScàqD9‘k“)GS?s)Llj\n",
            "HmâF1blâOyjv:à^w[lwk“)U7dfa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "08763a63-2446-41cf-f519-765a9a316b62"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3633055686950684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "2df178c6-70f1-4205-9523-386ba12291fc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "he akik, Winoby\n",
            "l mis Teint ancaty estovoowsore otos wowie he s gnd tim\n",
            "w]Jabe\n",
            "pasher t atotedivint.”\n",
            "odeasster wan nanlinicoust ales he arger bacoutwhat,\n",
            "“iss jercth. he thexpt Swan owow ond r s the stiry hearf  se ecervenenerber”\n",
            "\n",
            "tovemine “a atovent I aban f Bist,” o\n",
            "or silinisssg-ive ke\n",
            "wiba orase?”\n",
            "at yPecy t; terer leroty, daitepe s\n",
            "t  sind aby dose oo  AGe--.\n",
            "\n",
            "ns in, song w  f cthe  bemaneishithier wnndoutor. gus asso o\n",
            "Mrlen\n",
            "\n",
            "hiveJan istavend I hig  is cl rdeore d\n",
            "hep thimot n I  s  towe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQy_wJfqqpBC"
      },
      "execution_count": 51,
      "outputs": []
    }
  ]
}